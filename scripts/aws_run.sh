#!/usr/bin/env bash
#
# Sync the repository to the EC2 runner, execute a make target under micromamba,
# and pull back reports (including run.json metadata).
#
# Usage:
#   scripts/aws_run.sh <make-target> [<make-args>...]
#
# Environment:
#   INSTANCE_DNS   – required, EC2 public DNS
#   KEY_PATH       – required, SSH private key
#   INSTANCE_USER  – optional, default: ubuntu
#   REMOTE_ROOT    – optional, default: /home/<user>/fjs-dealias-portfolio
#   MICROMAMBA_ENV – optional, default: fjs
#   MICROMAMBA_ROOT_PREFIX – optional, default: $HOME/.local/share/mamba
#   AWS_REPORTS_DIR – optional, default: reports/aws
#   SKIP_INSTALL   – optional, set to 1 to skip pip install -e
#   SKIP_REPORT_SYNC – optional, set to 1 to skip pulling remote reports
#   RUN_ID         – optional, override autogenerated run id
#
# Example:
#   INSTANCE_DNS=ec2-3-236-225-54.compute-1.amazonaws.com \
#   KEY_PATH=~/.ssh/mateo-us-east-1-ec2-2025 \
#   scripts/aws_run.sh rc-lite

set -euo pipefail

[[ -n "${DEBUG_AWS_RUN:-}" ]] && set -x

usage() {
  cat <<'EOF'
Usage: scripts/aws_run.sh <make-target> [<make-args>...]

Required environment:
  INSTANCE_DNS   – EC2 public DNS name
  KEY_PATH       – SSH private key for the instance

Optional environment:
  INSTANCE_USER          – SSH user (default: ubuntu)
  REMOTE_ROOT            – Remote repo path (default: /home/<user>/fjs-dealias-portfolio)
  MICROMAMBA_ENV         – Micromamba environment name (default: fjs)
  MICROMAMBA_ROOT_PREFIX – Micromamba root (default: $HOME/.local/share/mamba)
  AWS_REPORTS_DIR        – Local directory to store synced reports (default: reports/aws)
  SKIP_INSTALL           – Skip pip install -e when set to 1 (default: 0)
  SKIP_REPORT_SYNC       – Skip pulling reports back when set to 1 (default: 0)
  RUN_ID                 – Override autogenerated run identifier
  EXEC_MODE              – Execution mode passed to runners (deterministic|throughput, default: deterministic)
  SHARD_ID               – Optional shard identifier forwarded to make (default: unset)
  SHARD_MANIFEST         – Optional shard manifest path forwarded to make
EOF
}

if [[ $# -lt 1 ]]; then
  usage >&2
  exit 1
fi

require_env() {
  local var="$1"
  if [[ -z "${!var:-}" ]]; then
    echo "error: environment variable '$var' must be set" >&2
    exit 1
  fi
}

require_env "INSTANCE_DNS"
require_env "KEY_PATH"

TARGET="$1"
shift
MAKE_ARGS=("$@")

REPO_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

INSTANCE_USER="${INSTANCE_USER:-ubuntu}"
MICROMAMBA_ENV="${MICROMAMBA_ENV:-fjs}"
MICROMAMBA_ROOT_PREFIX="${MICROMAMBA_ROOT_PREFIX:-/home/${INSTANCE_USER}/.local/share/mamba}"
REMOTE_ROOT_DEFAULT="/home/${INSTANCE_USER}/fjs-dealias-portfolio"
REMOTE_ROOT="${REMOTE_ROOT:-$REMOTE_ROOT_DEFAULT}"
AWS_REPORTS_DIR="${AWS_REPORTS_DIR:-reports/aws}"
SKIP_INSTALL="${SKIP_INSTALL:-0}"
SKIP_REPORT_SYNC="${SKIP_REPORT_SYNC:-0}"
EXEC_MODE="${EXEC_MODE:-deterministic}"

RUN_ID="${RUN_ID:-$(date -u +%Y%m%dT%H%M%SZ)}"
GIT_SHA="$(cd "$REPO_ROOT" && git rev-parse HEAD)"

SSH_COMMON_OPTS=(
  -o "StrictHostKeyChecking=accept-new"
  -o "BatchMode=yes"
  -i "$KEY_PATH"
)
SSH_TARGET="${INSTANCE_USER}@${INSTANCE_DNS}"

RSYNC_EXCLUDES=(
  "--exclude=.git/"
  "--exclude=.mypy_cache/"
  "--exclude=.pytest_cache/"
  "--exclude=.ruff_cache/"
  "--exclude=.venv/"
  "--exclude=__pycache__/"
  "--exclude=reports/"
  "--exclude=dist/"
  "--exclude=build/"
)

MAKE_CMD_ARGS=("make" "$TARGET")
if ((${#MAKE_ARGS[@]})); then
  MAKE_CMD_ARGS+=("${MAKE_ARGS[@]}")
fi
MAKE_CMD_STR=""
for arg in "${MAKE_CMD_ARGS[@]}"; do
  if [[ -z "$MAKE_CMD_STR" ]]; then
    MAKE_CMD_STR="$(printf '%q' "$arg")"
  else
    MAKE_CMD_STR+=" $(printf '%q' "$arg")"
  fi
done
MAKE_CMD_B64="$(printf '%s' "$MAKE_CMD_STR" | base64)"

echo "==> Remote: $INSTANCE_DNS (target: $TARGET, run id: $RUN_ID)"

echo "==> Ensuring remote directory '$REMOTE_ROOT'"
ssh "${SSH_COMMON_OPTS[@]}" "$SSH_TARGET" "mkdir -p '$REMOTE_ROOT'"

echo "==> Rsync repository to remote"
rsync -az --delete "${RSYNC_EXCLUDES[@]}" -e "ssh ${SSH_COMMON_OPTS[*]}" \
  --rsync-path="env RSYNC_MAX_ALLOC=2G rsync" \
  "$REPO_ROOT"/ "$SSH_TARGET":"$REMOTE_ROOT"/

REMOTE_ENV_PREFIX=""
REMOTE_ENV_KEYS=(
  "RUN_ID"
  "REMOTE_ROOT"
  "MICROMAMBA_ENV"
  "MICROMAMBA_ROOT_PREFIX"
  "MAKE_CMD_B64"
  "GIT_SHA"
  "SKIP_INSTALL"
  "TARGET_NAME"
  "MONITOR_INTERVAL"
  "EXEC_MODE"
  "SHARD_ID"
  "SHARD_MANIFEST"
)

for key in "${REMOTE_ENV_KEYS[@]}"; do
  set +u
  value="${!key}"
  set -u
  REMOTE_ENV_PREFIX+=$(printf "%s=%q " "$key" "$value")
done

REMOTE_SCRIPT=$(cat <<'EOSH'
set -euo pipefail

decode_cmd() {
  printf '%s' "$MAKE_CMD_B64" | base64 --decode
}

REPO_ROOT="$REMOTE_ROOT"
RUNS_DIR="$REPO_ROOT/reports/runs"
RUN_DIR="$RUNS_DIR/$RUN_ID"
mkdir -p "$RUN_DIR"

INSTALL_LOG="$RUN_DIR/pip_install.log"
RUN_LOG="$RUN_DIR/make_${RUN_ID}.log"
METRICS_JSON="$RUN_DIR/metrics.jsonl"
METRICS_SUMMARY="$RUN_DIR/metrics_summary.json"
PROGRESS_JSON="$RUN_DIR/progress.jsonl"

cd "$REPO_ROOT"

THREAD_MODE="${EXEC_MODE:-deterministic}"
THREAD_COUNT="${EXEC_THREADS:-}"
if [[ -z "$THREAD_COUNT" ]]; then
  if [[ "$THREAD_MODE" == "throughput" ]]; then
    THREAD_COUNT="${EXEC_THREADS_THROUGHPUT:-4}"
  else
    THREAD_COUNT="${EXEC_THREADS_DETERMINISTIC:-1}"
  fi
fi
export EXEC_MODE="$THREAD_MODE"
for var in OMP_NUM_THREADS OPENBLAS_NUM_THREADS MKL_NUM_THREADS NUMEXPR_NUM_THREADS BLIS_NUM_THREADS VECLIB_MAXIMUM_THREADS; do
  export "$var"="$THREAD_COUNT"
done
export PYTHONHASHSEED=0
export MPLBACKEND=Agg

if [[ "${SKIP_INSTALL}" != "1" ]]; then
  if ! MICROMAMBA_ROOT_PREFIX="$MICROMAMBA_ROOT_PREFIX" micromamba run -n "$MICROMAMBA_ENV" \
      python -m pip install --upgrade pip wheel setuptools >/dev/null 2>&1; then
    echo "warning: pip bootstrap upgrade failed" >&2
  fi
  set +e
  MICROMAMBA_ROOT_PREFIX="$MICROMAMBA_ROOT_PREFIX" micromamba run -n "$MICROMAMBA_ENV" \
      python -m pip install -e "$REPO_ROOT[dev]" >"$INSTALL_LOG" 2>&1
  INSTALL_STATUS=$?
  set -e
  if [[ $INSTALL_STATUS -ne 0 ]]; then
    echo "pip install failed; see $INSTALL_LOG" >&2
    tail -n 200 "$INSTALL_LOG" >&2 || true
    exit $INSTALL_STATUS
  fi
fi

MONITOR_INTERVAL="${MONITOR_INTERVAL:-5}"

MAKE_CMD="$(decode_cmd)"
export MAKE_CMD_STR="$MAKE_CMD"
set +e
MICROMAMBA_ROOT_PREFIX="$MICROMAMBA_ROOT_PREFIX" micromamba run -n "$MICROMAMBA_ENV" \
  python tools/run_monitor.py \
    --command-b64 "$MAKE_CMD_B64" \
    --metrics-out "$METRICS_JSON" \
    --summary-out "$METRICS_SUMMARY" \
    --log-out "$RUN_LOG" \
    --progress-out "$PROGRESS_JSON" \
    --interval "$MONITOR_INTERVAL" \
    --tag "$TARGET_NAME" \
    --cwd "$REPO_ROOT"
RUN_STATUS=$?
set -e
export RUN_STATUS="$RUN_STATUS"

MICROMAMBA_ROOT_PREFIX="$MICROMAMBA_ROOT_PREFIX" micromamba run -n "$MICROMAMBA_ENV" python - <<'PY'
import hashlib
import json
import os
import platform
import subprocess
from datetime import datetime, timezone
from pathlib import Path

repo_root = Path(os.environ["REMOTE_ROOT"])
run_dir = repo_root / "reports" / "runs" / os.environ["RUN_ID"]
run_json = run_dir / "run.json"
run_json.parent.mkdir(parents=True, exist_ok=True)

metrics_summary = {}
summary_path = run_dir / "metrics_summary.json"
if summary_path.exists():
    try:
        metrics_summary = json.loads(summary_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError:
        metrics_summary = {}

def _cpu():
    try:
        out = subprocess.check_output(["lscpu", "--json"], text=True)
        payload = json.loads(out)
        return {item["field"].strip(":"): item["data"] for item in payload.get("lscpu", [])}
    except Exception:
        return {}

def _blas():
    try:
        import numpy as np  # type: ignore
        info = np.__config__.get_info("openblas_info")
        if info:
            return {"library": "openblas", "config": info}
        info = np.__config__.get_info("blas_opt_info")
        return {"library": "blas_opt", "config": info}
    except Exception:
        return {}

def _thread_caps():
    keys = [
        "OMP_NUM_THREADS",
        "OPENBLAS_NUM_THREADS",
        "MKL_NUM_THREADS",
        "NUMEXPR_NUM_THREADS",
        "BLIS_NUM_THREADS",
        "VECLIB_MAXIMUM_THREADS",
    ]
    return {key: os.environ.get(key) for key in keys if os.environ.get(key)}

def _dataset_digest(path: Path) -> dict[str, object]:
    if not path.exists():
        return {"exists": False}
    h = hashlib.sha256()
    with path.open("rb") as handle:
        for chunk in iter(lambda: handle.read(1 << 20), b""):
            h.update(chunk)
    return {
        "exists": True,
        "sha256": h.hexdigest(),
        "size_bytes": path.stat().st_size,
    }

def _data_hashes():
    targets = [
        "data/returns_daily.csv",
        "data/returns_balanced_weekly.parquet",
        "data/prices_daily.csv",
    ]
    results = {}
    for rel in targets:
        results[rel] = _dataset_digest(repo_root / rel)
    return results

def _instance_type():
    env_type = os.environ.get("INSTANCE_TYPE")
    if env_type:
        return env_type
    try:
        import urllib.request
        req = urllib.request.Request("http://169.254.169.254/latest/meta-data/instance-type", method="GET")
        with urllib.request.urlopen(req, timeout=0.25) as resp:
            return resp.read().decode("utf-8")
    except Exception:
        return None

metadata = {
    "run_id": os.environ.get("RUN_ID"),
    "git_sha": os.environ.get("GIT_SHA"),
    "target": os.environ.get("TARGET_NAME"),
    "make_cmd": (metrics_summary.get("command") or os.environ.get("MAKE_CMD_STR")),
    "runner": {
        "hostname": platform.node(),
        "platform": platform.platform(),
        "python": platform.python_version(),
        "cpu": _cpu(),
        "blas": _blas(),
        "thread_caps": _thread_caps(),
        "instance_type": _instance_type(),
    },
    "timing": {
        "start_utc": metrics_summary.get("start_utc"),
        "end_utc": metrics_summary.get("end_utc"),
        "duration_seconds": float(metrics_summary.get("duration_seconds", 0.0)),
    },
    "status": int(metrics_summary.get("return_code", os.environ.get("RUN_STATUS", "0"))),
    "data_hashes": _data_hashes(),
    "generated_at": datetime.now(timezone.utc).isoformat(),
    "exec_mode": os.environ.get("EXEC_MODE"),
    "shard_id": os.environ.get("SHARD_ID"),
    "shard_manifest": os.environ.get("SHARD_MANIFEST"),
}

if metrics_summary:
    metadata["metrics_summary"] = metrics_summary
    metadata["metrics_jsonl"] = str((run_dir / "metrics.jsonl").relative_to(repo_root))
    if (run_dir / "progress.jsonl").exists():
        metadata["progress_jsonl"] = str((run_dir / "progress.jsonl").relative_to(repo_root))

run_json.write_text(json.dumps(metadata, indent=2), encoding="utf-8")
PY

exit "$RUN_STATUS"
EOSH
)

echo "==> Executing remote make command"
ssh "${SSH_COMMON_OPTS[@]}" "$SSH_TARGET" \
  "${REMOTE_ENV_PREFIX} bash -s" <<<"$REMOTE_SCRIPT"
REMOTE_EXIT=$?

if [[ "$SKIP_REPORT_SYNC" != "1" ]]; then
  LOCAL_SYNC_DIR="$REPO_ROOT/$AWS_REPORTS_DIR/$RUN_ID"
  if [[ -n "${SHARD_ID:-}" ]]; then
    LOCAL_SYNC_DIR="$LOCAL_SYNC_DIR/shard_${SHARD_ID}"
  fi
  mkdir -p "$LOCAL_SYNC_DIR"
  echo "==> Syncing reports back to $LOCAL_SYNC_DIR"
  ssh "${SSH_COMMON_OPTS[@]}" "$SSH_TARGET" "test -d '$REMOTE_ROOT/reports'" && \
    rsync -az -e "ssh ${SSH_COMMON_OPTS[*]}" \
      "$SSH_TARGET":"$REMOTE_ROOT/reports/" "$LOCAL_SYNC_DIR"/ || \
    echo "warning: remote reports directory missing; skipping sync" >&2
fi

if [[ $REMOTE_EXIT -ne 0 ]]; then
  echo "Remote run failed with exit code $REMOTE_EXIT" >&2
fi

exit "$REMOTE_EXIT"
